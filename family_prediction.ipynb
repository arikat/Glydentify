{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import EsmForSequenceClassification, AdamW, AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/aarya/uniref50.231020.80gaps.v2.csv\", sep=\",\")\n",
    "df = df.drop_duplicates(subset=['Sequence'], keep='first')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(df[\"Sequence\"].str.upper())\n",
    "y = list(df[\"Family\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifiable parameters\n",
    "\n",
    "# Name of ESM2 model be one listed on: https://github.com/facebookresearch/esm\n",
    "esm_model_name = \"facebook/esm2_t12_35M_UR50D\"\n",
    "# Suffix added to all files\n",
    "file_naming = \"35M_t12_famv2\"\n",
    "lr=3e-4\n",
    "weight_decay = 1e-5 # Regularization strength for L2\n",
    "l1_lambda = 1e-5  # Regularization strength for L1\n",
    "batch_size = 32\n",
    "num_epochs = 160\n",
    "step_size = 15 # epoch step size for LR scheduler at which point LR is multiplied by gamma to produce a smoother learning rate\n",
    "gamma = 0.6\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_series = pd.Series(y)\n",
    "class_counts = y_series.value_counts()\n",
    "single_occurrence_classes = class_counts[class_counts == 1].index\n",
    "mask = ~y_series.isin(single_occurrence_classes)\n",
    "y_filtered = y_series[mask].tolist()\n",
    "x_filtered = [x[i] for i in range(len(x)) if mask.iloc[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# First, split the data into 70% training and 30% temp\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_filtered, y_filtered, test_size=0.3, shuffle=True, stratify=y_filtered, random_state=42)\n",
    "\n",
    "# Then, split the temp data into 50% validation and 50% testing\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "# Tokenizer for sequences and label encoder for labels\n",
    "tokenizer_esm = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)  # Fit the label encoder on the entire label set 'y'\n",
    "\n",
    "# Transform the labels for train, val, and test sets\n",
    "encoded_labels_train = label_encoder.transform(y_train)\n",
    "encoded_labels_val = label_encoder.transform(y_val)\n",
    "encoded_labels_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "labels_train = torch.tensor(encoded_labels_train)\n",
    "labels_val = torch.tensor(encoded_labels_val)\n",
    "labels_test = torch.tensor(encoded_labels_test)\n",
    "\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "# Save the labels so they can be reconstructed in other files\n",
    "with open('/home/aarya/35M_t12_famv1_labels.pkl', 'wb') as file:\n",
    "    pickle.dump(label_encoder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formats data for model (train set) \n",
    "encoded_train_inputs = tokenizer_esm(x_train, padding=True, truncation=True, max_length=450, return_tensors=\"pt\")\n",
    "train_input_ids = encoded_train_inputs[\"input_ids\"]\n",
    "train_attention_mask = encoded_train_inputs[\"attention_mask\"]\n",
    "encoded_train_labels = label_encoder.transform(y_train)\n",
    "train_labels = torch.tensor(encoded_train_labels)\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "\n",
    "# Formats data for model (validation set) \n",
    "encoded_val_inputs = tokenizer_esm(x_val, padding=True, truncation=True, max_length=450, return_tensors=\"pt\")\n",
    "val_input_ids = encoded_val_inputs[\"input_ids\"]\n",
    "val_attention_mask = encoded_val_inputs[\"attention_mask\"]\n",
    "encoded_val_labels = label_encoder.transform(y_val)\n",
    "val_labels = torch.tensor(encoded_val_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels)\n",
    "\n",
    "# Formats data for model (test set) \n",
    "encoded_test_inputs = tokenizer_esm(x_test, padding=True, truncation=True, max_length=450, return_tensors=\"pt\")\n",
    "test_input_ids = encoded_test_inputs[\"input_ids\"]\n",
    "test_attention_mask = encoded_test_inputs[\"attention_mask\"]\n",
    "encoded_test_labels = label_encoder.transform(y_test)\n",
    "test_labels = torch.tensor(encoded_test_labels)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "# Defining Focal Loss here. Use focal lost for multi-label classification problems (eg 1 sequence might be categorized under multiple labels)\n",
    "# Use cross-entropy loss (default) when one label captures one sequence. (EG: Sequence is X family only).\n",
    "\n",
    "# A future consideration may be implemetnation of focal loss for family prediction, where GTs may belong to an overarching family (GT2), but may also be classified into subfamilies (GT2-exo, GT2_bact, etc).\n",
    "# The development of this dataset is not difficult, but the success of a simple cross entropy single label classification approach serves as the foundation for that implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(y)  # Assuming y is your label array\n",
    "print(\"Unique labels in dataset:\", unique_labels)\n",
    "print(\"Number of unique labels:\", len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# CSV function to track various aspects about model training \n",
    "def write_to_csv(epoch, train_accuracy, avg_train_loss, val_accuracy, avg_val_loss, filename='training_progress.csv'):\n",
    "    with open(filename, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([epoch, train_accuracy, avg_train_loss, val_accuracy, avg_val_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EsmForSequenceClassification.from_pretrained(esm_model_name, num_labels=len(set(y)))\n",
    "model = model.to(device)\n",
    "\n",
    "# Can be changed if needed (AdamW has worked well but others may be better)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)  # Decays lr every X epochs by multiplying the lr by Y\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize variables for best validation accuracy and corresponding state dict\n",
    "best_val_acc = 0\n",
    "best_state_dict = None\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_test_acc = 0\n",
    "patience = 10  # Number of epochs with no improvement after which training will be stopped\n",
    "no_improve = 0  # Track epochs with no improvement\n",
    "prev_val_loss = float('inf')  # Set an initial large previous validation loss\n",
    "class_weights = class_weights.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights) # balances class weights for labels\n",
    "\n",
    "# Initially, create the file and write headers\n",
    "with open('training_progress.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Epoch', 'Training Accuracy', 'Training Loss', 'Validation Accuracy', 'Validation Loss'])\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    progress_bar = tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\", position=0, leave=False)\n",
    "\n",
    "    train_loss = 0\n",
    "    num_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "\n",
    "    # Train loop, trains each batch\n",
    "    for batch in train_dataloader:\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Ensure gradients are zeroed \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_input_ids.to(device), attention_mask=batch_attention_mask.to(device))\n",
    "\n",
    "        # Calculate loss with criterion\n",
    "        loss = criterion(outputs.logits, batch_labels.to(device))\n",
    "\n",
    "        # Add L1 regularization\n",
    "        l1_loss = 0\n",
    "        for param in model.parameters():\n",
    "            l1_loss += param.abs().sum()\n",
    "        loss += l1_lambda * l1_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss/accuracy tracking\n",
    "        train_loss += loss.item()\n",
    "        _, predicted_labels_train = torch.max(outputs.logits, dim=1)\n",
    "        num_correct_train += (predicted_labels_train == batch_labels).sum().item()\n",
    "        total_samples_train += len(batch_labels)\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "\n",
    "    train_accuracy = num_correct_train / total_samples_train\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "            outputs = model(batch_input_ids.to(device), attention_mask=batch_attention_mask.to(device), labels=batch_labels.to(device))\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted_labels = torch.max(logits, dim=1)\n",
    "            correct_val += (predicted_labels == batch_labels.to(device)).sum().item()\n",
    "            total_val += len(batch_labels)\n",
    "            \n",
    "    val_accuracy = correct_val / total_val\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # Check if this epoch resulted in a better validation accuracy\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        best_state_dict = model.state_dict()\n",
    "\n",
    "\n",
    "    scheduler.step()\n",
    "    # Print statistics after the epoch, write to csv, and save model\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Train Accuracy: {train_accuracy:.4f} - Validation Loss: {avg_val_loss:.4f} - Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    write_to_csv(epoch+1, train_accuracy, avg_train_loss, val_accuracy, avg_val_loss)\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch+1}_{file_naming}.pth\") # Save best epoch (if don't overwrite prev epochs: epoch_{epoch+1}_)\n",
    "\n",
    "    # Early stopping based on validation loss\n",
    "    if avg_val_loss < prev_val_loss:\n",
    "        prev_val_loss = avg_val_loss\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if no_improve == patience:\n",
    "        print(f\"Early stopping after {patience} epochs with no improvement.\")\n",
    "        break\n",
    "\n",
    "    # Stop if learning rate is too small\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    if current_lr < 1e-6:  # This threshold can be adjusted\n",
    "        print(f\"Stopping due to learning rate becoming too small: {current_lr}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize variables for tracking test loss and accuracy\n",
    "test_loss = 0\n",
    "num_correct_test = 0\n",
    "total_samples_test = 0\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Testing loop\n",
    "model.eval()  # Switch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_input_ids.to(device), \n",
    "                        attention_mask=batch_attention_mask.to(device), \n",
    "                        labels=batch_labels.to(device))\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Update test loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Get predictions and update the number of correct predictions and total samples\n",
    "        _, predicted_labels_test = torch.max(logits, dim=1)\n",
    "        num_correct_test += (predicted_labels_test == batch_labels.to(device)).sum().item()\n",
    "        total_samples_test += len(batch_labels)\n",
    "\n",
    "# Calculate average test loss and test accuracy\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "test_accuracy = num_correct_test / total_samples_test\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "with open('/home/aarya/35M_t12_famv1_labels.pkl', 'rb') as file:\n",
    "    label_encoder = pickle.load(file)\n",
    "\n",
    "\n",
    "# Define the model architecture (make sure it's the same as the one you trained)\n",
    "model = EsmForSequenceClassification.from_pretrained(esm_model_name, num_labels=len(set(y)))\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('/home/aarya/best_model_35M_t12_famv1.pth'))\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Initialize variables for tracking test loss and accuracy\n",
    "test_loss = 0\n",
    "num_correct_test = 0\n",
    "total_samples_test = 0\n",
    "\n",
    "# Lists to store true and predicted labels\n",
    "true_labels_int = []\n",
    "predicted_labels_int = []\n",
    "\n",
    "# Testing loop\n",
    "model.eval()  # Switch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_input_ids.to(device), \n",
    "                        attention_mask=batch_attention_mask.to(device), \n",
    "                        labels=batch_labels.to(device))\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Update test loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Get predictions and update the number of correct predictions and total samples\n",
    "        _, predicted_labels_test = torch.max(logits, dim=1)\n",
    "        num_correct_test += (predicted_labels_test == batch_labels.to(device)).sum().item()\n",
    "        total_samples_test += len(batch_labels)\n",
    "\n",
    "        # Store true and predicted labels\n",
    "        true_labels_int.extend(batch_labels.tolist())\n",
    "        predicted_labels_int.extend(predicted_labels_test.tolist())\n",
    "\n",
    "# Calculate average test loss and test accuracy\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "test_accuracy = num_correct_test / total_samples_test\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Convert integer labels back to original string labels\n",
    "true_labels_str = label_encoder.inverse_transform(true_labels_int)\n",
    "predicted_labels_str = label_encoder.inverse_transform(predicted_labels_int)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(true_labels_str, predicted_labels_str, labels=label_encoder.classes_)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm_norm[np.isnan(cm_norm)] = 0  # Replace NaNs with zeros if needed\n",
    "\n",
    "# Convert to percentages\n",
    "cm_percentage = cm_norm * 100\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(100, 100))\n",
    "sns.heatmap(cm_percentage, annot=True, fmt='.1f', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label = \"Train\")\n",
    "plt.plot(test_losses, label = \"Test\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accuracies, label = \"Train\")\n",
    "plt.plot(test_accuracies, label = \"Test\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt_pred",
   "language": "python",
   "name": "gt_pred"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
