{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aarya/mambaforge/envs/gt_pred/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import gc\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import EsmForSequenceClassification, AdamW, AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UniRef ID</th>\n",
       "      <th>Family</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A8J7T852</td>\n",
       "      <td>GT2-B3GntL</td>\n",
       "      <td>vmdevmtllsdtnwesnlatvrthrnqlcdllgvqkpcvvaklfyh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E3NE16</td>\n",
       "      <td>GT2-B3GntL</td>\n",
       "      <td>myefdvsVIIPARNAEKFLRETLNGLLAQTAveNARIEICLADDGS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A814LGG6</td>\n",
       "      <td>GT2-B3GntL</td>\n",
       "      <td>msidvsIIMPVRNAAQWLNETFESLMKQIIENiNIELSIYNDGSSa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A9Q0MDR6</td>\n",
       "      <td>GT2-B3GntL</td>\n",
       "      <td>masnnlvsVIICVRNSEKWIEECLNSITEQTYDGPIEVSIFDDGSS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A8J5XDW0</td>\n",
       "      <td>GT2-B3GntL</td>\n",
       "      <td>asepkpaqqhlsgarapgpaprsartadarvgldarartppaasrw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201363</th>\n",
       "      <td>A0A9D6NVK0</td>\n",
       "      <td>GT116</td>\n",
       "      <td>mspIVIYTAifGPKGELHEPLHpDPRLDYvcFTDRTDLRSEVFEvr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201364</th>\n",
       "      <td>A0A7Z0SGI3</td>\n",
       "      <td>GT116</td>\n",
       "      <td>msnITVFTTvfGNTDPLHEPASAGGARfvcFTDQQIRSSKWEiVQM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201365</th>\n",
       "      <td>A0A1H3REG2</td>\n",
       "      <td>GT116</td>\n",
       "      <td>mykenkIVVYTAimGNYETIKEIHIREPnidYVLFTDNPNVNSATW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201366</th>\n",
       "      <td>A0A917CI68</td>\n",
       "      <td>GT116</td>\n",
       "      <td>mirgIIYTAitNGYDTVKTPKTDQTLPmvcFQNSTGGYtNRWESKK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201367</th>\n",
       "      <td>A0A965L983</td>\n",
       "      <td>GT116</td>\n",
       "      <td>mitvytslvgdrdeclpnqvpegvhfeffrdcydkFKDDRRNSRPH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201368 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         UniRef ID       Family  \\\n",
       "0       A0A8J7T852   GT2-B3GntL   \n",
       "1           E3NE16   GT2-B3GntL   \n",
       "2       A0A814LGG6   GT2-B3GntL   \n",
       "3       A0A9Q0MDR6   GT2-B3GntL   \n",
       "4       A0A8J5XDW0   GT2-B3GntL   \n",
       "...            ...          ...   \n",
       "201363  A0A9D6NVK0        GT116   \n",
       "201364  A0A7Z0SGI3        GT116   \n",
       "201365  A0A1H3REG2        GT116   \n",
       "201366  A0A917CI68        GT116   \n",
       "201367  A0A965L983        GT116   \n",
       "\n",
       "                                                 Sequence  \n",
       "0       vmdevmtllsdtnwesnlatvrthrnqlcdllgvqkpcvvaklfyh...  \n",
       "1       myefdvsVIIPARNAEKFLRETLNGLLAQTAveNARIEICLADDGS...  \n",
       "2       msidvsIIMPVRNAAQWLNETFESLMKQIIENiNIELSIYNDGSSa...  \n",
       "3       masnnlvsVIICVRNSEKWIEECLNSITEQTYDGPIEVSIFDDGSS...  \n",
       "4       asepkpaqqhlsgarapgpaprsartadarvgldarartppaasrw...  \n",
       "...                                                   ...  \n",
       "201363  mspIVIYTAifGPKGELHEPLHpDPRLDYvcFTDRTDLRSEVFEvr...  \n",
       "201364  msnITVFTTvfGNTDPLHEPASAGGARfvcFTDQQIRSSKWEiVQM...  \n",
       "201365  mykenkIVVYTAimGNYETIKEIHIREPnidYVLFTDNPNVNSATW...  \n",
       "201366  mirgIIYTAitNGYDTVKTPKTDQTLPmvcFQNSTGGYtNRWESKK...  \n",
       "201367  mitvytslvgdrdeclpnqvpegvhfeffrdcydkFKDDRRNSRPH...  \n",
       "\n",
       "[201368 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/aarya/uniref50.231020.80gaps.v2.csv\", sep=\",\")\n",
    "df = df.drop_duplicates(subset=['Sequence'], keep='first')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(df[\"Sequence\"].str.upper())\n",
    "y = list(df[\"Family\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifiable parameters\n",
    "\n",
    "# Name of ESM2 model be one listed on: https://github.com/facebookresearch/esm\n",
    "esm_model_name = \"facebook/esm2_t12_35M_UR50D\"\n",
    "# Suffix added to all files\n",
    "file_naming = \"35M_t12_famv2\"\n",
    "lr=5e-5\n",
    "weight_decay = 5e-3 # Regularization strength for L2\n",
    "l1_lambda = 5e-3  # Regularization strength for L1\n",
    "batch_size = 64\n",
    "num_epochs = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_series = pd.Series(y)\n",
    "class_counts = y_series.value_counts()\n",
    "single_occurrence_classes = class_counts[class_counts == 1].index\n",
    "mask = ~y_series.isin(single_occurrence_classes)\n",
    "y_filtered = y_series[mask].tolist()\n",
    "x_filtered = [x[i] for i in range(len(x)) if mask.iloc[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data into 70% training and 30% temp\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x_filtered, y_filtered, test_size=0.3, shuffle=True, stratify=y_filtered, random_state=42)\n",
    "\n",
    "# Then, split the temp data into 50% validation and 50% testing\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, shuffle=True, random_state=42)\n",
    "\n",
    "# Tokenizer for sequences and label encoder for labels\n",
    "tokenizer_esm = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)  # Fit the label encoder on the entire label set 'y'\n",
    "\n",
    "# Transform the labels for train, val, and test sets\n",
    "encoded_labels_train = label_encoder.transform(y_train)\n",
    "encoded_labels_val = label_encoder.transform(y_val)\n",
    "encoded_labels_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "labels_train = torch.tensor(encoded_labels_train)\n",
    "labels_val = torch.tensor(encoded_labels_val)\n",
    "labels_test = torch.tensor(encoded_labels_test)\n",
    "\n",
    "\n",
    "# Save the labels so they can be reconstructed in other files\n",
    "with open('/home/aarya/35M_t12_famv1_labels.pkl', 'wb') as file:\n",
    "    pickle.dump(label_encoder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formats data for model (train set) \n",
    "encoded_train_inputs = tokenizer_esm(x_train, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "train_input_ids = encoded_train_inputs[\"input_ids\"]\n",
    "train_attention_mask = encoded_train_inputs[\"attention_mask\"]\n",
    "encoded_train_labels = label_encoder.transform(y_train)\n",
    "train_labels = torch.tensor(encoded_train_labels)\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "\n",
    "# Formats data for model (validation set) \n",
    "encoded_val_inputs = tokenizer_esm(x_val, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "val_input_ids = encoded_val_inputs[\"input_ids\"]\n",
    "val_attention_mask = encoded_val_inputs[\"attention_mask\"]\n",
    "encoded_val_labels = label_encoder.transform(y_val)\n",
    "val_labels = torch.tensor(encoded_val_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels)\n",
    "\n",
    "# Formats data for model (test set) \n",
    "encoded_test_inputs = tokenizer_esm(x_test, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "test_input_ids = encoded_test_inputs[\"input_ids\"]\n",
    "test_attention_mask = encoded_test_inputs[\"attention_mask\"]\n",
    "encoded_test_labels = label_encoder.transform(y_test)\n",
    "test_labels = torch.tensor(encoded_test_labels)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss\n",
    "\n",
    "# Defining Focal Loss here. Use focal lost for multi-label classification problems (eg 1 sequence might be categorized under multiple labels)\n",
    "# Use cross-entropy loss (default) when one label captures one sequence. (EG: Sequence is X family only).\n",
    "\n",
    "# A future consideration may be implemetnation of focal loss for family prediction, where GTs may belong to an overarching family (GT2), but may also be classified into subfamilies (GT2-exo, GT2_bact, etc).\n",
    "# The development of this dataset is not difficult, but the success of a simple cross entropy single label classification approach serves as the foundation for that implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in dataset: {' GT92', ' GT7', ' GT14', ' GT2-Csl2', ' GT43', ' GT34', ' GT12', ' GT77', ' GT2-Exo', ' GT25', ' GT6', 'GT116', ' GT2-Bact_Chlor2', ' GT24', ' GT16', ' GT2-Bact_DPM', ' GT75', ' GT40', ' GT2-Bact2', ' GT55', ' GT2-Cel_bre3', ' GT31', ' GT54', ' GT13', ' GT2-Bact_EpsO', ' GT21', ' GT2-DPM_like', ' GT2-Cslens', ' GT2-B3GntL', ' GT2-CesA1', ' GT15', ' GT17', ' GT49', ' GT88', ' GT2-Bact_Chlor1', ' GT64', ' GT82', ' GT27', ' GT2-Chitin', ' GT62', ' GT60', ' GT8', ' GT81', ' GT2-Bact', ' GT2-Bact_CHS', ' GT2-DPG', ' GT45', ' GT84', ' GT32', ' GT2-CesA2', ' GT78', ' GT67', ' GT2-HAS'}\n",
      "Number of unique labels: 53\n"
     ]
    }
   ],
   "source": [
    "unique_labels = set(y)  # Assuming y is your label array\n",
    "print(\"Unique labels in dataset:\", unique_labels)\n",
    "print(\"Number of unique labels:\", len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/160:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 839/2203 [15:45<25:33,  1.12s/it, Loss=8e+3]   "
     ]
    }
   ],
   "source": [
    "model = EsmForSequenceClassification.from_pretrained(esm_model_name, num_labels=len(set(y)))\n",
    "model = model.to(device)\n",
    "\n",
    "# Can be changed if needed (AdamW has worked well but others may be better)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=80, gamma=0.6)  # Decays lr every 40 epochs by multiplying the lr by 0.6\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize variables for best validation accuracy and corresponding state dict\n",
    "best_val_acc = 0\n",
    "best_state_dict = None\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_test_acc = 0\n",
    "patience = 10  # Number of epochs with no improvement after which training will be stopped\n",
    "no_improve = 0  # Track epochs with no improvement\n",
    "prev_val_loss = float('inf')  # Set an initial large previous validation loss\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    progress_bar = tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\", position=0, leave=False)\n",
    "\n",
    "    train_loss = 0\n",
    "    num_correct_train = 0\n",
    "    total_samples_train = 0\n",
    "    # Train loop, trains each batch\n",
    "    for batch in train_dataloader:\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "        # Ensure gradients are zeroed \n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(batch_input_ids.to(device), attention_mask=batch_attention_mask.to(device), labels=batch_labels.to(device))\n",
    "        # Calc losses\n",
    "        l1_loss = 0\n",
    "        for param in model.parameters():\n",
    "            l1_loss += param.abs().sum()\n",
    "        \n",
    "        loss = outputs.loss + l1_lambda * l1_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss/accuracy tracking\n",
    "        train_loss += loss.item()\n",
    "        _, predicted_labels_train = torch.max(outputs.logits, dim=1)\n",
    "        num_correct_train += (predicted_labels_train == batch_labels.to(device)).sum().item()\n",
    "        total_samples_train += len(batch_labels)\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
    "\n",
    "\n",
    "    train_accuracy = num_correct_train / total_samples_train\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "            outputs = model(batch_input_ids.to(device), attention_mask=batch_attention_mask.to(device), labels=batch_labels.to(device))\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted_labels = torch.max(logits, dim=1)\n",
    "            correct_val += (predicted_labels == batch_labels.to(device)).sum().item()\n",
    "            total_val += len(batch_labels)\n",
    "            \n",
    "    val_accuracy = correct_val / total_val\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    # Check if this epoch resulted in a better validation accuracy\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        best_state_dict = model.state_dict()\n",
    "\n",
    "\n",
    "    scheduler.step()\n",
    "    # Print statistics after the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Train Accuracy: {train_accuracy:.4f} - Validation Loss: {avg_val_loss:.4f} - Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch+1}_{file_naming}.pth\") # Save best epoch (if don't overwrite prev epochs: epoch_{epoch+1}_)\n",
    "\n",
    "    # Early stopping based on validation loss\n",
    "    if avg_val_loss < prev_val_loss:\n",
    "        prev_val_loss = avg_val_loss\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if no_improve == patience:\n",
    "        print(f\"Early stopping after {patience} epochs with no improvement.\")\n",
    "        break\n",
    "\n",
    "    # Stop if learning rate is too small\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    if current_lr < 1e-6:  # This threshold can be adjusted\n",
    "        print(f\"Stopping due to learning rate becoming too small: {current_lr}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize variables for tracking test loss and accuracy\n",
    "test_loss = 0\n",
    "num_correct_test = 0\n",
    "total_samples_test = 0\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Testing loop\n",
    "model.eval()  # Switch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_input_ids.to(device), \n",
    "                        attention_mask=batch_attention_mask.to(device), \n",
    "                        labels=batch_labels.to(device))\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Update test loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Get predictions and update the number of correct predictions and total samples\n",
    "        _, predicted_labels_test = torch.max(logits, dim=1)\n",
    "        num_correct_test += (predicted_labels_test == batch_labels.to(device)).sum().item()\n",
    "        total_samples_test += len(batch_labels)\n",
    "\n",
    "# Calculate average test loss and test accuracy\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "test_accuracy = num_correct_test / total_samples_test\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "with open('/home/aarya/35M_t12_famv1_labels.pkl', 'rb') as file:\n",
    "    label_encoder = pickle.load(file)\n",
    "\n",
    "\n",
    "# Define the model architecture (make sure it's the same as the one you trained)\n",
    "model = EsmForSequenceClassification.from_pretrained(esm_model_name, num_labels=len(set(y)))\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load('/home/aarya/best_model_35M_t12_famv1.pth'))\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Initialize variables for tracking test loss and accuracy\n",
    "test_loss = 0\n",
    "num_correct_test = 0\n",
    "total_samples_test = 0\n",
    "\n",
    "# Lists to store true and predicted labels\n",
    "true_labels_int = []\n",
    "predicted_labels_int = []\n",
    "\n",
    "# Testing loop\n",
    "model.eval()  # Switch model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch_input_ids.to(device), \n",
    "                        attention_mask=batch_attention_mask.to(device), \n",
    "                        labels=batch_labels.to(device))\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Update test loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Get predictions and update the number of correct predictions and total samples\n",
    "        _, predicted_labels_test = torch.max(logits, dim=1)\n",
    "        num_correct_test += (predicted_labels_test == batch_labels.to(device)).sum().item()\n",
    "        total_samples_test += len(batch_labels)\n",
    "\n",
    "        # Store true and predicted labels\n",
    "        true_labels_int.extend(batch_labels.tolist())\n",
    "        predicted_labels_int.extend(predicted_labels_test.tolist())\n",
    "\n",
    "# Calculate average test loss and test accuracy\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "test_accuracy = num_correct_test / total_samples_test\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Convert integer labels back to original string labels\n",
    "true_labels_str = label_encoder.inverse_transform(true_labels_int)\n",
    "predicted_labels_str = label_encoder.inverse_transform(predicted_labels_int)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(true_labels_str, predicted_labels_str, labels=label_encoder.classes_)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples in each class)\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm_norm[np.isnan(cm_norm)] = 0  # Replace NaNs with zeros if needed\n",
    "\n",
    "# Convert to percentages\n",
    "cm_percentage = cm_norm * 100\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(100, 100))\n",
    "sns.heatmap(cm_percentage, annot=True, fmt='.1f', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label = \"Train\")\n",
    "plt.plot(test_losses, label = \"Test\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accuracies, label = \"Train\")\n",
    "plt.plot(test_accuracies, label = \"Test\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_650M_2-e7.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on 200k Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"testingv2.csv\", sep=\",\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(df[\"sequence\"])\n",
    "y = list(df[\"GT-A Family\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_inputs = tokenizer(x, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "test_input_ids = encoded_test_inputs[\"input_ids\"]\n",
    "test_attention_mask = encoded_test_inputs[\"attention_mask\"]\n",
    "encoded_test_labels = label_encoder.transform(y)\n",
    "test_labels = torch.tensor(encoded_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "num_correct = 0\n",
    "total_samples = 0\n",
    "misclassified_samples = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "\n",
    "        outputs = model(batch_input_ids.to(device), attention_mask=batch_attention_mask.to(device), labels=batch_labels.to(device))\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted_labels = torch.max(logits, dim=1)\n",
    "        num_correct += (predicted_labels == batch_labels.to(device)).sum().item()\n",
    "        total_samples += len(batch_labels)\n",
    "\n",
    "        # Track misclassified samples\n",
    "        misclassified_mask = predicted_labels\n",
    "        misclassified_indices = torch.nonzero(misclassified_mask).squeeze().tolist()\n",
    "\n",
    "        if type(misclassified_indices) == int:\n",
    "            misclassified_indices = [misclassified_indices]\n",
    "        if misclassified_indices is not None:\n",
    "            misclassified_samples.extend([(label.item(), predicted.item()) for label, predicted in zip(batch_labels[misclassified_indices],\n",
    "                                                                                                                predicted_labels[misclassified_indices])])\n",
    "\n",
    "        print(f\"Progress {i+1}/{len(test_dataloader)}\", end=\"\\r\")\n",
    "\n",
    "test_accuracy = num_correct / total_samples\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Avg. Test Loss: {avg_test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = [data[0] for data in misclassified_samples]\n",
    "predicted_labels = [data[1] for data in misclassified_samples]\n",
    "\n",
    "true_label_name = label_encoder.inverse_transform(true_label)\n",
    "predicted_labels_name = label_encoder.inverse_transform(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"True_Label\": true_label_name, \"Predicted\": predicted_labels_name})\n",
    "df.to_csv(\"misclassified_gt_650M_2-e7.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chumby",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
