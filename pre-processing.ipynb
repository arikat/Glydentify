{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing Notebook -- Create a dataset for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "\n",
    "df = pd.DataFrame(columns=['Uniprot_ID', 'Family', 'Sequence'])\n",
    "\n",
    "# Open the fasta file\n",
    "with open('/home/aarya/Downloads/GTseq/galA.reformat.newline.a2m', 'r') as file:\n",
    "    for record in SeqIO.parse(file, 'fasta'):\n",
    "        header_parts = record.description.split('|')\n",
    "        uniprot_id = header_parts[0].strip()\n",
    "        family = header_parts[1].strip() if len(header_parts) > 1 else None\n",
    "\n",
    "        new_row = pd.DataFrame({\n",
    "            'Uniprot_ID': [uniprot_id],\n",
    "            'Family': [family],\n",
    "            'Sequence': [str(record.seq)]\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# Save as CSV for training\n",
    "df.to_csv('/home/aarya/Downloads/GTseq/galA.reformat.newline.8.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset + donor CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "\n",
    "df = pd.DataFrame(columns=['Uniprot_ID', 'Donor', 'Family', 'Sequence'])\n",
    "\n",
    "# Open the fasta file\n",
    "with open('/home/aarya/Documents/paper3/DL_donor_specificity_class/training4/testing3v1_8435_n.9.fa', 'r') as file:\n",
    "    for record in SeqIO.parse(file, 'fasta'):\n",
    "        header_parts = record.description.split('|')\n",
    "        uniprot_id = header_parts[0].strip()\n",
    "        family = header_parts[1].strip() if len(header_parts) > 1 else None\n",
    "        donor = header_parts[2].strip() if len(header_parts) > 2 else None\n",
    "\n",
    "        new_row = pd.DataFrame({\n",
    "            'Uniprot_ID': [uniprot_id],\n",
    "            'Donor': [donor],\n",
    "            'Family': [family],\n",
    "            'Sequence': [str(record.seq)]\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# Save as CSV for training\n",
    "df.to_csv('/home/aarya/Documents/paper3/DL_donor_specificity_class/training4/testing3v1_8435_n.9.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parses Uniprot and Adds family to uniprot ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_header(header):\n",
    "    uniprot_id = header.split()[0][1:]\n",
    "    match = re.search(r'profile=([\\w-]+)', header)\n",
    "    family = match.group(1) if match else 'Unknown'\n",
    "    return f\">{uniprot_id} | {family}\"\n",
    "\n",
    "with open('/home/aarya/Documents/paper3/family_prediction/uniref50.231020.nogaps.trimmed.v2.a2m', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open('/home/aarya/Documents/paper3/family_prediction/uniref50.231020.nogaps.trimmed.reformatted.v2.a2m', 'w') as f:\n",
    "    for line in lines:\n",
    "        if line.startswith('>'):\n",
    "            header = line.strip()\n",
    "            profile = re.search(r'profile=(\\S+)', header).group(1)\n",
    "            header_parts = header.split(' ')\n",
    "            truncated_header = header_parts[0]\n",
    "            f.write(f\"{truncated_header} | {profile}\\n\")\n",
    "        else:\n",
    "            f.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim Inserts from Original Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210781"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import re\n",
    "\n",
    "def trim_fasta_sequence(record, max_insert=50):\n",
    "    # Split the sequence into conserved and insert regions\n",
    "    conserved_insert_regions = re.split(r'([A-Z\\-]+)', str(record.seq))\n",
    "\n",
    "    # Trim the insert regions\n",
    "    trimmed_sequence = ''\n",
    "    for region in conserved_insert_regions:\n",
    "        if region.islower():\n",
    "            trimmed_sequence += region[-max_insert:] if len(region) > max_insert else region\n",
    "        else:\n",
    "            trimmed_sequence += region\n",
    "\n",
    "    # Update the record's sequence\n",
    "    record.seq = Seq(trimmed_sequence)\n",
    "    return record\n",
    "\n",
    "def count_uppercase_chars(sequence):\n",
    "    # Count the number of uppercase characters in the sequence\n",
    "    return sum(1 for char in sequence if char.isupper())\n",
    "\n",
    "# Read the sequences from a fasta file\n",
    "fasta_sequences = list(SeqIO.parse('/home/aarya/Documents/paper3/family_prediction/uniref50.231020.nogaps.fasta.cfa', 'fasta'))\n",
    "\n",
    "# Filter out sequences with less than 100 uppercase characters\n",
    "filtered_sequences = [record for record in fasta_sequences if count_uppercase_chars(record.seq) >= 150]\n",
    "\n",
    "# Trim the sequences\n",
    "trimmed_sequences = [trim_fasta_sequence(record) for record in filtered_sequences]\n",
    "\n",
    "# Write the trimmed sequences to a new fasta file\n",
    "SeqIO.write(trimmed_sequences, '/home/aarya/Documents/paper3/family_prediction/uniref50.231020.nogaps.trimmed.v2.cfa', 'fasta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly sample 10K sequences from the original sequence set then produce training and testing datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import random\n",
    "\n",
    "sequences = list(SeqIO.parse('/home/aarya/Documents/paper3/DL_donor_specificity_class/training2/33k_donors.fasta', 'fasta'))\n",
    "\n",
    "families = {}\n",
    "\n",
    "for seq in sequences:\n",
    "    header = seq.description\n",
    "    family_name = header.split('|')[1]\n",
    "    if family_name in families:\n",
    "        families[family_name].append(seq)\n",
    "    else:\n",
    "        families[family_name] = [seq]\n",
    "\n",
    "random_sample = []\n",
    "picked_sequences_ids = set()\n",
    "samples_per_family = 5000 // len(families)\n",
    "\n",
    "for family_name, seqs in families.items():\n",
    "    if len(seqs) > samples_per_family:\n",
    "        sample = random.sample(seqs, samples_per_family)\n",
    "    else:\n",
    "        sample = seqs\n",
    "    random_sample.extend(sample)\n",
    "    picked_sequences_ids.update(seq.id for seq in sample)\n",
    "\n",
    "while len(random_sample) < 5000:\n",
    "    remaining_samples = 5000 - len(random_sample)\n",
    "    additional_sequences = [seq for family in families.values() for seq in family if seq.id not in picked_sequences_ids]\n",
    "    if len(additional_sequences) > remaining_samples:\n",
    "        new_samples = random.sample(additional_sequences, remaining_samples)\n",
    "        random_sample.extend(new_samples)\n",
    "        picked_sequences_ids.update(seq.id for seq in new_samples)\n",
    "    else:\n",
    "        random_sample.extend(additional_sequences)\n",
    "        picked_sequences_ids.update(seq.id for seq in additional_sequences)\n",
    "\n",
    "with open('/home/aarya/Documents/paper3/DL_donor_specificity_class/training2/trainingv2_5000.fa', 'w') as f:\n",
    "    SeqIO.write(random_sample, f, 'fasta')\n",
    "\n",
    "# Create the testing dataset with the remaining sequences\n",
    "remaining_sequences = [seq for seq in sequences if seq.id not in picked_sequences_ids]\n",
    "\n",
    "with open('/home/aarya/Documents/paper3/DL_donor_specificity_class/training2/testingv2_30000.fa', 'w') as f:\n",
    "    SeqIO.write(remaining_sequences, f, 'fasta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief change to above to include donors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import random\n",
    "\n",
    "sequences = list(SeqIO.parse('/home/aarya/Documents/paper3/DL_donor_specificity_class/training2/33k_donors.fasta', 'fasta'))\n",
    "\n",
    "families = {}\n",
    "\n",
    "for seq in sequences:\n",
    "    header = seq.description.split('|')\n",
    "    family_name = header[1]\n",
    "    substrate = header[2]\n",
    "\n",
    "    if family_name not in families:\n",
    "        families[family_name] = {}\n",
    "\n",
    "    if substrate in families[family_name]:\n",
    "        families[family_name][substrate].append(seq)\n",
    "    else:\n",
    "        families[family_name][substrate] = [seq]\n",
    "\n",
    "random_sample = []\n",
    "picked_sequences_ids = set()\n",
    "samples_per_family = 1000 // len(families)\n",
    "\n",
    "for family_name, substrates in families.items():\n",
    "    samples_per_substrate = samples_per_family // len(substrates)\n",
    "    for substrate, seqs in substrates.items():\n",
    "        if len(seqs) > samples_per_substrate:\n",
    "            sample = random.sample(seqs, samples_per_substrate)\n",
    "        else:\n",
    "            sample = seqs\n",
    "        random_sample.extend(sample)\n",
    "        picked_sequences_ids.update(seq.id for seq in sample)\n",
    "\n",
    "while len(random_sample) < 1000:\n",
    "    remaining_samples = 1000 - len(random_sample)\n",
    "    additional_sequences = [seq for family in families.values() for substrate in family.values() for seq in substrate if seq.id not in picked_sequences_ids]\n",
    "    if len(additional_sequences) > remaining_samples:\n",
    "        new_samples = random.sample(additional_sequences, remaining_samples)\n",
    "        random_sample.extend(new_samples)\n",
    "        picked_sequences_ids.update(seq.id for seq in new_samples)\n",
    "    else:\n",
    "        random_sample.extend(additional_sequences)\n",
    "        picked_sequences_ids.update(seq.id for seq in additional_sequences)\n",
    "\n",
    "with open('/home/aarya/Documents/paper3/DL_donor_specificity_class/training3/training3v1_1000.fa', 'w') as f:\n",
    "    SeqIO.write(random_sample, f, 'fasta')\n",
    "\n",
    "# Create the testing dataset with the remaining sequences\n",
    "remaining_sequences = [seq for seq in sequences if seq.id not in picked_sequences_ids]\n",
    "\n",
    "with open('/home/aarya/Documents/paper3/DL_donor_specificity_class/training3/testing3v1_36000.fa', 'w') as f:\n",
    "    SeqIO.write(remaining_sequences, f, 'fasta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33K sequences organize csv into fasta containing donor info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load csv file\n",
    "data = pd.read_csv(\"./train_test_v3.csv\")\n",
    "\n",
    "# Open the new fasta file\n",
    "with open(\"./train_test_v3.fasta\", \"w\") as f:\n",
    "    # Iterate over rows in the DataFrame\n",
    "    for index, row in data.iterrows():\n",
    "        # Write fasta header\n",
    "        f.write(f\">{row['GT-A Family']}\\n\") #{row['uniprot']}|{row['GT-A Family']}|{row['donor']}\n",
    "        # Write sequence\n",
    "        f.write(f\"{row['sequence']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from Bio import SeqIO\n",
    "\n",
    "# # Define a new DataFrame with the required columns\n",
    "# df = pd.DataFrame(columns=['Family', 'Sequence'])\n",
    "\n",
    "# # Specify the path to your fasta file\n",
    "# fasta_file_path = '/home/aarya/Documents/paper3/family_prediction/uniref50.231020.nogaps.trimmed.reformatted.v2.80.a2m'\n",
    "\n",
    "# # Open the fasta file\n",
    "# with open(fasta_file_path, 'r') as file:\n",
    "#     for record in SeqIO.parse(file, 'fasta'):\n",
    "#         # The header is just the family name\n",
    "#         family = record.id.strip()\n",
    "\n",
    "#         # Append a new row to the DataFrame\n",
    "#         new_row = pd.DataFrame({\n",
    "#             'Family': [family],\n",
    "#             'Sequence': [str(record.seq)]\n",
    "#         })\n",
    "#         df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# # Specify the path where you want to save your CSV file\n",
    "# csv_file_path = '/home/aarya/Documents/paper3/family_prediction/uniref50.231020.80gaps.v2.csv'\n",
    "\n",
    "# # Save as CSV\n",
    "# df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Initialize a list to hold the data\n",
    "data = []\n",
    "\n",
    "# Specify the path to your fasta file\n",
    "fasta_file_path = '/home/aarya/Documents/paper3/family_prediction/uniref50.231020.nogaps.trimmed.reformatted.v2.80.a2m'\n",
    "\n",
    "# Open the fasta file\n",
    "with open(fasta_file_path, 'r') as file:\n",
    "    for record in SeqIO.parse(file, 'fasta'):\n",
    "        # Split the header to get UniRef ID and Family\n",
    "        header_parts = record.id.split('|')\n",
    "        uniref_id = header_parts[0].strip() if len(header_parts) > 0 else 'Unknown'\n",
    "        family = header_parts[1].strip() if len(header_parts) > 1 else 'Unknown'\n",
    "\n",
    "        # Append the data to the list\n",
    "        data.append({'UniRef ID': uniref_id, 'Family': family, 'Sequence': str(record.seq)})\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df = pd.DataFrame(data, columns=['UniRef ID', 'Family', 'Sequence'])\n",
    "\n",
    "# Specify the path where you want to save your CSV file\n",
    "csv_file_path = '/home/aarya/Documents/paper3/family_prediction/uniref50.231020.80gaps.v2.csv'\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('grady')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe4ca989064a99d95da564ec0947b0b13201ef548f8980923fe3cc3150848b38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
